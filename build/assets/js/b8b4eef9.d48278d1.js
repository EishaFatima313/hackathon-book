"use strict";(globalThis.webpackChunkrobotics_education_platform=globalThis.webpackChunkrobotics_education_platform||[]).push([[582],{4789(e,n,t){t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"vision-language-action/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Introduction","source":"@site/docs/vision-language-action/index.md","sourceDirName":"vision-language-action","slug":"/vision-language-action/","permalink":"/docs/vision-language-action/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docs","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","permalink":"/docs/ai-robot-brain/"}}');var s=t(4848),i=t(8453);const r={sidebar_position:4},l="Module 4: Vision-Language-Action (VLA)",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"1. What is VLA?",id:"1-what-is-vla",level:2},{value:"The Three Parts of VLA",id:"the-three-parts-of-vla",level:3},{value:"How VLA Works Together",id:"how-vla-works-together",level:3},{value:"Why VLA Matters",id:"why-vla-matters",level:3},{value:"Simple Example",id:"simple-example",level:3},{value:"2. Voice-to-Action: Speech-to-Text",id:"2-voice-to-action-speech-to-text",level:2},{value:"How Robots Hear",id:"how-robots-hear",level:3},{value:"Speech-to-Text Basics",id:"speech-to-text-basics",level:3},{value:"Using Speech Recognition in Python",id:"using-speech-recognition-in-python",level:3},{value:"Installing Speech Recognition",id:"installing-speech-recognition",level:3},{value:"Common Voice Commands for Robots",id:"common-voice-commands-for-robots",level:3},{value:"From Text to Action",id:"from-text-to-action",level:3},{value:"3. LLM: Converting Commands to Robot Steps",id:"3-llm-converting-commands-to-robot-steps",level:2},{value:"What is an LLM?",id:"what-is-an-llm",level:3},{value:"How LLMs Help Robots",id:"how-llms-help-robots",level:3},{value:"Using an LLM with Your Robot",id:"using-an-llm-with-your-robot",level:3},{value:"Example Output",id:"example-output",level:3},{value:"Mapping Steps to ROS 2 Actions",id:"mapping-steps-to-ros-2-actions",level:3},{value:"Complete Voice-to-Steps Example",id:"complete-voice-to-steps-example",level:3},{value:"4. Computer Vision: Detecting Objects",id:"4-computer-vision-detecting-objects",level:2},{value:"How Robots See",id:"how-robots-see",level:3},{value:"Object Detection",id:"object-detection",level:3},{value:"Using YOLO for Object Detection",id:"using-yolo-for-object-detection",level:3},{value:"Installing YOLO",id:"installing-yolo",level:3},{value:"Common Objects YOLO Can Detect",id:"common-objects-yolo-can-detect",level:3},{value:"Getting Object Positions",id:"getting-object-positions",level:3},{value:"Vision Pipeline for Robot",id:"vision-pipeline-for-robot",level:3},{value:"5. ROS 2 Nodes and Actions",id:"5-ros-2-nodes-and-actions",level:2},{value:"ROS 2 Nodes Recap",id:"ros-2-nodes-recap",level:3},{value:"ROS 2 Actions",id:"ros-2-actions",level:3},{value:"Action Example: Navigate to Position",id:"action-example-navigate-to-position",level:3},{value:"Creating an Action Client",id:"creating-an-action-client",level:3},{value:"Putting It All Together: VLA Architecture",id:"putting-it-all-together-vla-architecture",level:3},{value:"6. Capstone Project: Humanoid Robot Assistant",id:"6-capstone-project-humanoid-robot-assistant",level:2},{value:"Project Overview",id:"project-overview",level:3},{value:"System Architecture",id:"system-architecture",level:3},{value:"Complete Code",id:"complete-code",level:3},{value:"Running the Capstone Project",id:"running-the-capstone-project",level:3},{value:"Testing Without Hardware",id:"testing-without-hardware",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"What&#39;s Next?",id:"whats-next",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Welcome to Module 4! In this module, you will learn how robots can:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"See"})," the world using cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Understand"})," human speech and commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Act"})," to complete tasks"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This combination is called ",(0,s.jsx)(n.strong,{children:"Vision-Language-Action"})," or ",(0,s.jsx)(n.strong,{children:"VLA"})," for short."]}),"\n",(0,s.jsx)(n.p,{children:"By the end of this module, you will build a capstone project: a humanoid robot that listens to your voice, plans its actions, navigates a room, finds an object, and picks it up!"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"1-what-is-vla",children:"1. What is VLA?"}),"\n",(0,s.jsx)(n.h3,{id:"the-three-parts-of-vla",children:"The Three Parts of VLA"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"VLA"})," stands for ",(0,s.jsx)(n.strong,{children:"Vision-Language-Action"}),". It is how smart robots understand and interact with the world."]}),"\n",(0,s.jsx)(n.p,{children:"Think of VLA like a human:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Part"}),(0,s.jsx)(n.th,{children:"What It Does"}),(0,s.jsx)(n.th,{children:"Like a Human"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Vision"})}),(0,s.jsx)(n.td,{children:"Sees objects, people, and places"}),(0,s.jsx)(n.td,{children:"Eyes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Language"})}),(0,s.jsx)(n.td,{children:"Understands words and commands"}),(0,s.jsx)(n.td,{children:"Ears + Brain"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Action"})}),(0,s.jsx)(n.td,{children:"Moves and does tasks"}),(0,s.jsx)(n.td,{children:"Hands + Legs"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"how-vla-works-together",children:"How VLA Works Together"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'[Human speaks] \u2192 [Robot hears] \u2192 [Robot understands] \u2192 [Robot sees] \u2192 [Robot acts]\n   "Pick up      [Speech-to-     [LLM plans          [Camera        [Motors\n    the cup"]      text]           steps]              detects]       move]\n'})}),"\n",(0,s.jsx)(n.h3,{id:"why-vla-matters",children:"Why VLA Matters"}),"\n",(0,s.jsx)(n.p,{children:"Before VLA, robots could only do pre-programmed tasks. They could not understand new commands or adapt to new situations."}),"\n",(0,s.jsx)(n.p,{children:"With VLA, robots can:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand new commands they never heard before"}),"\n",(0,s.jsx)(n.li,{children:"Work in rooms they have never seen"}),"\n",(0,s.jsx)(n.li,{children:"Handle objects in different positions"}),"\n",(0,s.jsx)(n.li,{children:"Learn from mistakes"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"simple-example",children:"Simple Example"}),"\n",(0,s.jsxs)(n.p,{children:["Imagine you say: ",(0,s.jsx)(n.strong,{children:'"Bring me the red ball"'})]}),"\n",(0,s.jsx)(n.p,{children:"A VLA robot:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hears"})," your words (Language)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Understands"})," it needs to find a red ball and bring it (Language + Planning)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Looks"})," around for something red and round (Vision)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Moves"})," to the ball, grabs it, and brings it to you (Action)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"2-voice-to-action-speech-to-text",children:"2. Voice-to-Action: Speech-to-Text"}),"\n",(0,s.jsx)(n.h3,{id:"how-robots-hear",children:"How Robots Hear"}),"\n",(0,s.jsx)(n.p,{children:"Robots do not have ears like humans. Instead, they use:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["A ",(0,s.jsx)(n.strong,{children:"microphone"})," to capture sound"]}),"\n",(0,s.jsxs)(n.li,{children:["A ",(0,s.jsx)(n.strong,{children:"speech-to-text"})," system to convert sound to words"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"speech-to-text-basics",children:"Speech-to-Text Basics"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Speech-to-text"})," (also called ",(0,s.jsx)(n.strong,{children:"STT"}),") converts spoken words into text that a computer can understand."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'[You speak] \u2192 [Microphone] \u2192 [Audio signal] \u2192 [STT System] \u2192 [Text]\n"Move forward"                    (sound waves)              "move forward"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"using-speech-recognition-in-python",children:"Using Speech Recognition in Python"}),"\n",(0,s.jsx)(n.p,{children:"Python has easy-to-use speech recognition libraries. Here is a simple example:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\n\ndef listen_to_command():\n    """Listen for voice command and return text"""\n    recognizer = sr.Recognizer()\n    \n    with sr.Microphone() as source:\n        print("Listening...")\n        # Adjust for ambient noise\n        recognizer.adjust_for_ambient_noise(source)\n        \n        # Listen for audio\n        audio = recognizer.listen(source)\n        \n        try:\n            # Convert speech to text using Google\'s free API\n            command = recognizer.recognize_google(audio)\n            print(f"You said: {command}")\n            return command.lower()\n        except sr.UnknownValueError:\n            print("Sorry, I did not understand")\n            return None\n        except sr.RequestError:\n            print("Could not connect to speech service")\n            return None\n\n# Test the function\nif __name__ == "__main__":\n    command = listen_to_command()\n    if command:\n        print(f"Received command: {command}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"installing-speech-recognition",children:"Installing Speech Recognition"}),"\n",(0,s.jsx)(n.p,{children:"To use speech recognition, install the required packages:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install SpeechRecognition\npip install pyaudio\n"})}),"\n",(0,s.jsx)(n.h3,{id:"common-voice-commands-for-robots",children:"Common Voice Commands for Robots"}),"\n",(0,s.jsx)(n.p,{children:"Here are some simple commands your robot might understand:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Command"}),(0,s.jsx)(n.th,{children:"What Robot Does"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Move forward"'}),(0,s.jsx)(n.td,{children:"Drives forward"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Turn left"'}),(0,s.jsx)(n.td,{children:"Rotates left"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Stop"'}),(0,s.jsx)(n.td,{children:"Halts all motion"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Find the cup"'}),(0,s.jsx)(n.td,{children:"Looks for a cup"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Pick it up"'}),(0,s.jsx)(n.td,{children:"Grabs the object"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Bring it to me"'}),(0,s.jsx)(n.td,{children:"Returns with object"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"from-text-to-action",children:"From Text to Action"}),"\n",(0,s.jsxs)(n.p,{children:["Once the robot has the text, it needs to understand what to do. This is where ",(0,s.jsx)(n.strong,{children:"Large Language Models (LLMs)"})," come in."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"3-llm-converting-commands-to-robot-steps",children:"3. LLM: Converting Commands to Robot Steps"}),"\n",(0,s.jsx)(n.h3,{id:"what-is-an-llm",children:"What is an LLM?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LLM"})," stands for ",(0,s.jsx)(n.strong,{children:"Large Language Model"}),". It is an AI that understands and generates human language."]}),"\n",(0,s.jsx)(n.p,{children:"Examples of LLMs:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPT-4"}),"\n",(0,s.jsx)(n.li,{children:"Claude"}),"\n",(0,s.jsx)(n.li,{children:"Llama"}),"\n",(0,s.jsx)(n.li,{children:"Gemini"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"how-llms-help-robots",children:"How LLMs Help Robots"}),"\n",(0,s.jsxs)(n.p,{children:["When you say ",(0,s.jsx)(n.strong,{children:'"Clean the room"'}),", the LLM breaks it down into steps:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'[Input] "Clean the room"\n   \u2193\n[LLM thinks about what cleaning means]\n   \u2193\n[Output] Steps:\n1. Find trash on the floor\n2. Pick up each piece of trash\n3. Put trash in the bin\n4. Find dirty dishes\n5. Pick up dishes\n6. Take dishes to kitchen\n'})}),"\n",(0,s.jsx)(n.h3,{id:"using-an-llm-with-your-robot",children:"Using an LLM with Your Robot"}),"\n",(0,s.jsx)(n.p,{children:"Here is how to use an LLM to convert commands into robot actions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\n\ndef command_to_steps(command):\n    """Use LLM to convert command into robot steps"""\n    \n    # Create a prompt for the LLM\n    prompt = f"""\n    Convert this robot command into simple steps.\n    Each step should be one action the robot can do.\n    \n    Command: "{command}"\n    \n    Steps (use format: 1. [action]):\n    """\n    \n    # Call the LLM (using OpenAI as example)\n    response = openai.ChatCompletion.create(\n        model="gpt-3.5-turbo",\n        messages=[\n            {"role": "system", "content": "You are a robot planning assistant."},\n            {"role": "user", "content": prompt}\n        ]\n    )\n    \n    # Get the steps from the response\n    steps_text = response.choices[0].message.content\n    return steps_text\n\n# Example usage\ncommand = "Clean the room"\nsteps = command_to_steps(command)\nprint(steps)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-output",children:"Example Output"}),"\n",(0,s.jsxs)(n.p,{children:["For the command ",(0,s.jsx)(n.strong,{children:'"Clean the room"'}),", the LLM might output:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"1. Look around the room for objects on the floor\n2. Move to the nearest object\n3. Identify if the object is trash\n4. If trash, pick it up\n5. Move to the trash bin\n6. Drop the trash in the bin\n7. Repeat until no more trash is found\n"})}),"\n",(0,s.jsx)(n.h3,{id:"mapping-steps-to-ros-2-actions",children:"Mapping Steps to ROS 2 Actions"}),"\n",(0,s.jsx)(n.p,{children:"Each step needs to become a ROS 2 command:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"LLM Step"}),(0,s.jsx)(n.th,{children:"ROS 2 Action"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Look around"'}),(0,s.jsxs)(n.td,{children:["Rotate camera, publish to ",(0,s.jsx)(n.code,{children:"/camera"})," topic"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Move to object"'}),(0,s.jsxs)(n.td,{children:["Publish velocity to ",(0,s.jsx)(n.code,{children:"/cmd_vel"})," topic"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Pick it up"'}),(0,s.jsxs)(n.td,{children:["Call ",(0,s.jsx)(n.code,{children:"/arm/grasp"})," service"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:'"Drop in bin"'}),(0,s.jsxs)(n.td,{children:["Call ",(0,s.jsx)(n.code,{children:"/arm/release"})," service"]})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"complete-voice-to-steps-example",children:"Complete Voice-to-Steps Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\nimport openai\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\n\nclass VoiceCommandRobot(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_robot\')\n        \n        # Publisher for robot movement\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        \n        # Speech recognizer\n        self.recognizer = sr.Recognizer()\n        \n    def listen_and_act(self):\n        """Main loop: listen, understand, act"""\n        \n        # Step 1: Listen to voice command\n        with sr.Microphone() as source:\n            self.get_logger().info("Listening for command...")\n            audio = self.recognizer.listen(source)\n            \n        try:\n            # Step 2: Convert speech to text\n            command = self.recognizer.recognize_google(audio)\n            self.get_logger().info(f"Command: {command}")\n            \n            # Step 3: Use LLM to get steps\n            steps = self.get_steps_from_llm(command)\n            self.get_logger().info(f"Steps: {steps}")\n            \n            # Step 4: Execute each step\n            self.execute_steps(steps)\n            \n        except Exception as e:\n            self.get_logger().error(f"Error: {e}")\n    \n    def get_steps_from_llm(self, command):\n        """Get action steps from LLM"""\n        # Simplified - in real use, call actual LLM API\n        simple_commands = {\n            "move forward": ["move_forward"],\n            "turn left": ["turn_left"],\n            "stop": ["stop"],\n        }\n        return simple_commands.get(command.lower(), ["unknown"])\n    \n    def execute_steps(self, steps):\n        """Execute each step"""\n        for step in steps:\n            if step == "move_forward":\n                self.move_forward()\n            elif step == "turn_left":\n                self.turn_left()\n            elif step == "stop":\n                self.stop()\n    \n    def move_forward(self):\n        msg = Twist()\n        msg.linear.x = 0.5\n        self.cmd_vel_pub.publish(msg)\n        self.get_logger().info("Moving forward")\n    \n    def turn_left(self):\n        msg = Twist()\n        msg.angular.z = 0.5\n        self.cmd_vel_pub.publish(msg)\n        self.get_logger().info("Turning left")\n    \n    def stop(self):\n        msg = Twist()\n        msg.linear.x = 0.0\n        msg.angular.z = 0.0\n        self.cmd_vel_pub.publish(msg)\n        self.get_logger().info("Stopped")\n\ndef main():\n    rclpy.init()\n    robot = VoiceCommandRobot()\n    robot.listen_and_act()\n    rclpy.shutdown()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"4-computer-vision-detecting-objects",children:"4. Computer Vision: Detecting Objects"}),"\n",(0,s.jsx)(n.h3,{id:"how-robots-see",children:"How Robots See"}),"\n",(0,s.jsxs)(n.p,{children:["Robots use ",(0,s.jsx)(n.strong,{children:"cameras"})," to see the world. But seeing is not enough - the robot must ",(0,s.jsx)(n.strong,{children:"understand"})," what it sees."]}),"\n",(0,s.jsxs)(n.p,{children:["This is called ",(0,s.jsx)(n.strong,{children:"Computer Vision"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"object-detection",children:"Object Detection"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Object Detection"})," means finding and naming objects in an image."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'[Camera Image] \u2192 [Object Detection AI] \u2192 [Objects Found]\n                                          \u2193\n                                    "cup at x=100, y=200"\n                                    "chair at x=300, y=150"\n                                    "person at x=500, y=400"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"using-yolo-for-object-detection",children:"Using YOLO for Object Detection"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"YOLO"})," (You Only Look Once) is a popular, fast object detection system."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLO model\nmodel = YOLO(\'yolov8n.pt\')  # \'n\' = nano (smallest, fastest)\n\ndef detect_objects(image_path):\n    """Detect objects in an image"""\n    \n    # Load the image\n    image = cv2.imread(image_path)\n    \n    # Run detection\n    results = model(image)\n    \n    # Get detections\n    detections = results[0].boxes\n    \n    # Print results\n    for detection in detections:\n        class_id = int(detection.cls[0])\n        class_name = model.names[class_id]\n        confidence = float(detection.conf[0])\n        bbox = detection.xyxy[0].tolist()  # [x1, y1, x2, y2]\n        \n        print(f"Found: {class_name} ({confidence:.2f}) at {bbox}")\n    \n    return detections\n\n# Example usage\ndetections = detect_objects(\'room_photo.jpg\')\n'})}),"\n",(0,s.jsx)(n.h3,{id:"installing-yolo",children:"Installing YOLO"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install ultralytics\npip install opencv-python\n"})}),"\n",(0,s.jsx)(n.h3,{id:"common-objects-yolo-can-detect",children:"Common Objects YOLO Can Detect"}),"\n",(0,s.jsx)(n.p,{children:"YOLO can detect 80 common objects:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Category"}),(0,s.jsx)(n.th,{children:"Objects"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"People"}),(0,s.jsx)(n.td,{children:"person"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Vehicles"}),(0,s.jsx)(n.td,{children:"car, bicycle, motorcycle, bus, truck"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Animals"}),(0,s.jsx)(n.td,{children:"bird, cat, dog, horse"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Furniture"}),(0,s.jsx)(n.td,{children:"chair, couch, bed, dining table"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Items"}),(0,s.jsx)(n.td,{children:"cup, bottle, book, cell phone, laptop"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"getting-object-positions",children:"Getting Object Positions"}),"\n",(0,s.jsxs)(n.p,{children:["For a robot to interact with objects, it needs to know ",(0,s.jsx)(n.strong,{children:"where"})," they are in 3D space."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\n\ndef pixel_to_robot_coordinates(pixel_x, pixel_y, depth_value, camera_info):\n    \"\"\"Convert pixel position to robot coordinates\"\"\"\n    \n    # Camera intrinsics (from camera calibration)\n    fx = camera_info['fx']  # Focal length x\n    fy = camera_info['fy']  # Focal length y\n    cx = camera_info['cx']  # Principal point x\n    cy = camera_info['cy']  # Principal point y\n    \n    # Convert to 3D coordinates\n    z = depth_value  # Distance from camera\n    x = (pixel_x - cx) * z / fx\n    y = (pixel_y - cy) * z / fy\n    \n    return [x, y, z]\n\n# Example\ncamera_info = {'fx': 500, 'fy': 500, 'cx': 320, 'cy': 240}\nrobot_pos = pixel_to_robot_coordinates(400, 300, 2.0, camera_info)\nprint(f\"Object is at: {robot_pos} meters from robot\")\n"})}),"\n",(0,s.jsx)(n.h3,{id:"vision-pipeline-for-robot",children:"Vision Pipeline for Robot"}),"\n",(0,s.jsx)(n.p,{children:"Here is the complete vision pipeline:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import cv2\nfrom ultralytics import YOLO\nimport numpy as np\n\nclass RobotVision:\n    def __init__(self):\n        # Load object detection model\n        self.model = YOLO('yolov8n.pt')\n        \n        # Camera info (from calibration)\n        self.camera_info = {\n            'fx': 500, 'fy': 500,\n            'cx': 320, 'cy': 240\n        }\n    \n    def find_object(self, target_class, image, depth_image):\n        \"\"\"Find a specific object and return its 3D position\"\"\"\n        \n        # Detect objects\n        results = self.model(image)\n        detections = results[0].boxes\n        \n        # Search for target object\n        for detection in detections:\n            class_id = int(detection.cls[0])\n            class_name = self.model.names[class_id]\n            \n            if class_name == target_class:\n                # Get bounding box center\n                bbox = detection.xyxy[0].tolist()\n                center_x = int((bbox[0] + bbox[2]) / 2)\n                center_y = int((bbox[1] + bbox[3]) / 2)\n                \n                # Get depth at center\n                depth = depth_image[center_y, center_x]\n                \n                # Convert to 3D coordinates\n                position = self.pixel_to_3d(\n                    center_x, center_y, depth\n                )\n                \n                return {\n                    'found': True,\n                    'name': class_name,\n                    'position': position,\n                    'bbox': bbox\n                }\n        \n        return {'found': False}\n    \n    def pixel_to_3d(self, px, py, depth):\n        \"\"\"Convert pixel to 3D robot coordinates\"\"\"\n        fx = self.camera_info['fx']\n        fy = self.camera_info['fy']\n        cx = self.camera_info['cx']\n        cy = self.camera_info['cy']\n        \n        z = depth\n        x = (px - cx) * z / fx\n        y = (py - cy) * z / fy\n        \n        return [x, y, z]\n\n# Example usage\nvision = RobotVision()\nimage = cv2.imread('room.jpg')\ndepth = np.load('depth.npy')\n\nresult = vision.find_object('cup', image, depth)\nif result['found']:\n    print(f\"Cup found at: {result['position']}\")\nelse:\n    print(\"Cup not found\")\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"5-ros-2-nodes-and-actions",children:"5. ROS 2 Nodes and Actions"}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-nodes-recap",children:"ROS 2 Nodes Recap"}),"\n",(0,s.jsxs)(n.p,{children:["A ",(0,s.jsx)(n.strong,{children:"Node"})," is a program that does one job. Robots use many nodes working together."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[Camera Node] \u2500\u2500\u2500\u2500\u2500\u2500\u2192 [Vision Node] \u2500\u2500\u2500\u2500\u2192 [Action Node]\n     \u2193                      \u2193                    \u2193\n  publishes              processes            controls\n  images                 objects              motors\n"})}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-actions",children:"ROS 2 Actions"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Actions"})," are like services but for long-running tasks. They have:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": What to do"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback"}),": Progress updates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Result"}),": Final outcome"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"action-example-navigate-to-position",children:"Action Example: Navigate to Position"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\nfrom rclpy.node import Node\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\nimport time\n\nclass NavigationAction(Node):\n    def __init__(self):\n        super().__init__(\'navigation_action\')\n        \n        # Create action server\n        self.action_server = ActionServer(\n            self,\n            NavigateToPose,\n            \'navigate_to_pose\',\n            execute_callback=self.navigate_callback,\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback\n        )\n        \n        self.cmd_vel_pub = self.create_publisher(\n            Twist, \'/cmd_vel\', 10\n        )\n    \n    def goal_callback(self, goal_request):\n        """Accept or reject goal"""\n        self.get_logger().info("Received navigation goal")\n        return GoalResponse.ACCEPT\n    \n    def cancel_callback(self, goal_handle):\n        """Handle cancel request"""\n        self.get_logger().info("Canceling navigation")\n        return CancelResponse.ACCEPT\n    \n    async def navigate_callback(self, goal_handle):\n        """Execute navigation"""\n        self.get_logger().info("Executing navigation")\n        \n        # Get target position\n        target = goal_handle.request.pose.pose.position\n        \n        feedback_msg = NavigateToPose.Feedback()\n        \n        # Simple navigation loop\n        while True:\n            # Check if canceled\n            if goal_handle.is_cancel_requested:\n                goal_handle.canceled()\n                self.stop_robot()\n                return NavigateToPose.Result()\n            \n            # Get current position (simplified)\n            current_x = self.get_current_position()\n            \n            # Check if arrived\n            distance = abs(target.x - current_x)\n            if distance < 0.1:  # Within 10 cm\n                break\n            \n            # Move toward target\n            self.move_toward_target(target)\n            \n            # Send feedback\n            feedback_msg.distance_remaining = distance\n            goal_handle.publish_feedback(feedback_msg)\n            \n            # Wait a bit\n            await asyncio.sleep(0.1)\n        \n        # Success\n        goal_handle.succeed()\n        result = NavigateToPose.Result()\n        result.success = True\n        return result\n    \n    def get_current_position(self):\n        """Get robot\'s current position"""\n        # In real robot, get from odometry\n        return type(\'obj\', (object,), {\'x\': 0})()\n    \n    def move_toward_target(self, target):\n        """Move robot toward target"""\n        msg = Twist()\n        msg.linear.x = 0.5\n        self.cmd_vel_pub.publish(msg)\n    \n    def stop_robot(self):\n        """Stop robot"""\n        msg = Twist()\n        msg.linear.x = 0.0\n        msg.angular.z = 0.0\n        self.cmd_vel_pub.publish(msg)\n\ndef main():\n    rclpy.init()\n    nav_action = NavigationAction()\n    rclpy.spin(nav_action)\n    rclpy.shutdown()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"creating-an-action-client",children:"Creating an Action Client"}),"\n",(0,s.jsx)(n.p,{children:"The action client sends goals to the action server:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\n\nclass NavigationClient(Node):\n    def __init__(self):\n        super().__init__(\'navigation_client\')\n        \n        # Create action client\n        self.action_client = ActionClient(\n            self,\n            NavigateToPose,\n            \'navigate_to_pose\'\n        )\n    \n    def send_goal(self, x, y, z):\n        """Send navigation goal"""\n        \n        # Wait for server\n        self.action_client.wait_for_server()\n        \n        # Create goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = PoseStamped()\n        goal_msg.pose.pose.position.x = x\n        goal_msg.pose.pose.position.y = y\n        goal_msg.pose.pose.position.z = z\n        \n        # Send goal\n        self.get_logger().info(f"Sending goal: ({x}, {y}, {z})")\n        future = self.action_client.send_goal_async(\n            goal_msg,\n            feedback_callback=self.feedback_callback\n        )\n        \n        future.add_done_callback(self.goal_response_callback)\n    \n    def goal_response_callback(self, future):\n        """Handle goal response"""\n        goal_handle = future.result()\n        \n        if not goal_handle.accepted:\n            self.get_logger().info("Goal rejected")\n            return\n        \n        self.get_logger().info("Goal accepted")\n        \n        # Wait for result\n        result_future = goal_handle.get_result_async()\n        result_future.add_done_callback(self.result_callback)\n    \n    def feedback_callback(self, feedback_msg):\n        """Handle feedback"""\n        distance = feedback_msg.feedback.distance_remaining\n        self.get_logger().info(f"Distance remaining: {distance:.2f}m")\n    \n    def result_callback(self, future):\n        """Handle final result"""\n        result = future.result().result\n        self.get_logger().info(f"Navigation complete: {result.success}")\n\ndef main():\n    rclpy.init()\n    client = NavigationClient()\n    \n    # Send a goal\n    client.send_goal(1.0, 0.0, 0.0)\n    \n    rclpy.spin(client)\n    rclpy.shutdown()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"putting-it-all-together-vla-architecture",children:"Putting It All Together: VLA Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    VLA System Architecture                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502  \u2502  Voice   \u2502\u2500\u2500\u2500\u2192\u2502   LLM    \u2502\u2500\u2500\u2500\u2192\u2502  Vision  \u2502              \u2502\n\u2502  \u2502  Input   \u2502    \u2502  Planner \u2502    \u2502  System  \u2502              \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502       \u2193                \u2193                \u2193                   \u2502\n\u2502  Speech-to-       Command          Object                  \u2502\n\u2502  Text (STT)       Steps            Detection               \u2502\n\u2502                                      \u2193                     \u2502\n\u2502                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502                              \u2502  ROS 2       \u2502              \u2502\n\u2502                              \u2502  Actions     \u2502              \u2502\n\u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                                      \u2193                     \u2502\n\u2502                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502                              \u2502   Robot      \u2502              \u2502\n\u2502                              \u2502   Hardware   \u2502              \u2502\n\u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"6-capstone-project-humanoid-robot-assistant",children:"6. Capstone Project: Humanoid Robot Assistant"}),"\n",(0,s.jsx)(n.h3,{id:"project-overview",children:"Project Overview"}),"\n",(0,s.jsxs)(n.p,{children:["In this capstone, you will create a ",(0,s.jsx)(n.strong,{children:"humanoid robot"})," that:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Listens"})," to voice commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plans"})," actions using an LLM"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigates"})," to objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Detects"})," objects with vision"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Picks up"})," the object"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Humanoid Robot Assistant                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  1. Voice Command                                            \u2502\n\u2502     "Pick up the cup"                                        \u2502\n\u2502           \u2193                                                  \u2502\n\u2502  2. Speech-to-Text                                           \u2502\n\u2502     Text: "pick up the cup"                                  \u2502\n\u2502           \u2193                                                  \u2502\n\u2502  3. LLM Planning                                             \u2502\n\u2502     Steps: [find cup, navigate to cup, pick cup]             \u2502\n\u2502           \u2193                                                  \u2502\n\u2502  4. Vision System                                            \u2502\n\u2502     Detects: cup at position (1.5, 0.3, 0.8)                 \u2502\n\u2502           \u2193                                                  \u2502\n\u2502  5. Navigation                                               \u2502\n\u2502     Move to position (1.5, 0.3)                              \u2502\n\u2502           \u2193                                                  \u2502\n\u2502  6. Arm Control                                              \u2502\n\u2502     Reach, grasp, lift                                       \u2502\n\u2502           \u2193                                                  \u2502\n\u2502  7. Task Complete!                                           \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,s.jsx)(n.h3,{id:"complete-code",children:"Complete Code"}),"\n",(0,s.jsx)(n.p,{children:"Here is the complete VLA humanoid robot code:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'"""\nVLA Humanoid Robot Assistant\nCapstone Project for Module 4\n\nThis robot can:\n1. Listen to voice commands\n2. Understand commands using LLM\n3. Find objects with computer vision\n4. Navigate to objects\n5. Pick up objects with its arm\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom rclpy.executors import MultiThreadedExecutor\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom nav2_msgs.action import NavigateToPose\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nimport speech_recognition as sr\nfrom ultralytics import YOLO\nimport cv2\nimport numpy as np\nimport asyncio\n\n\nclass SpeechListener(Node):\n    """Node 1: Listen to voice commands"""\n    \n    def __init__(self):\n        super().__init__(\'speech_listener\')\n        \n        # Speech recognizer\n        self.recognizer = sr.Recognizer()\n        \n        # Publisher for recognized commands\n        self.command_pub = self.create_publisher(\n            String, \'/voice_command\', 10\n        )\n        \n        # Timer for listening (can be triggered by button in real robot)\n        self.timer = self.create_timer(5.0, self.listen_loop)\n        \n        self.listening = True\n    \n    def listen_loop(self):\n        """Continuously listen for commands"""\n        if not self.listening:\n            return\n        \n        try:\n            with sr.Microphone() as source:\n                self.get_logger().info("Listening...")\n                self.recognizer.adjust_for_ambient_noise(source)\n                audio = self.recognizer.listen(source, timeout=3)\n                \n                command = self.recognizer.recognize_google(audio)\n                self.get_logger().info(f"Heard: {command}")\n                \n                # Publish command\n                msg = String()\n                msg.data = command.lower()\n                self.command_pub.publish(msg)\n                \n        except sr.WaitTimeoutError:\n            pass  # No speech detected\n        except sr.UnknownValueError:\n            self.get_logger().info("Could not understand speech")\n        except Exception as e:\n            self.get_logger().error(f"Error: {e}")\n\n\nclass LLMPlanner(Node):\n    """Node 2: Convert commands to action steps using LLM"""\n    \n    def __init__(self):\n        super().__init__(\'llm_planner\')\n        \n        # Subscriber for voice commands\n        self.command_sub = self.create_subscription(\n            String, \'/voice_command\', self.command_callback, 10\n        )\n        \n        # Publisher for planned steps\n        self.steps_pub = self.create_publisher(\n            String, \'/planned_steps\', 10\n        )\n        \n        # Simple command mapping (replace with real LLM API)\n        self.command_map = {\n            "pick up the cup": "1.find_cup 2.navigate_to_cup 3.pick_object",\n            "bring me the bottle": "1.find_bottle 2.navigate_to_bottle 3.pick_object 4.return_to_user",\n            "clean the room": "1.find_trash 2.navigate_to_trash 3.pick_object 4.go_to_bin 5.release_object",\n            "move forward": "1.move_forward",\n            "turn left": "1.turn_left",\n            "turn right": "1.turn_right",\n            "stop": "1.stop",\n        }\n    \n    def command_callback(self, msg):\n        """Process voice command and generate steps"""\n        command = msg.data\n        self.get_logger().info(f"Planning for: {command}")\n        \n        # Get steps from LLM (simplified - use real LLM API here)\n        steps = self.command_map.get(command, "1.unknown_command")\n        \n        self.get_logger().info(f"Steps: {steps}")\n        \n        # Publish steps\n        steps_msg = String()\n        steps_msg.data = steps\n        self.steps_pub.publish(steps_msg)\n\n\nclass VisionSystem(Node):\n    """Node 3: Detect objects using computer vision"""\n    \n    def __init__(self):\n        super().__init__(\'vision_system\')\n        \n        # Load YOLO model\n        self.model = YOLO(\'yolov8n.pt\')\n        \n        # Subscriber for camera images\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n        \n        # Publisher for detected objects\n        self.object_pub = self.create_publisher(\n            String, \'/detected_objects\', 10\n        )\n        \n        # Camera info (from calibration)\n        self.camera_info = {\n            \'fx\': 500.0, \'fy\': 500.0,\n            \'cx\': 320.0, \'cy\': 240.0\n        }\n        \n        # Current target object\n        self.target_object = None\n        self.object_position = None\n    \n    def set_target(self, target_name):\n        """Set which object to look for"""\n        self.target_object = target_name\n        self.get_logger().info(f"Looking for: {target_name}")\n    \n    def image_callback(self, msg):\n        """Process camera image"""\n        if self.target_object is None:\n            return\n        \n        # Convert ROS image to OpenCV format\n        image = self.ros_to_cv2(msg)\n        \n        # Detect objects\n        results = self.model(image)\n        detections = results[0].boxes\n        \n        # Find target object\n        for detection in detections:\n            class_id = int(detection.cls[0])\n            class_name = self.model.names[class_id]\n            \n            if class_name == self.target_object:\n                # Get position\n                bbox = detection.xyxy[0].tolist()\n                center_x = int((bbox[0] + bbox[2]) / 2)\n                center_y = int((bbox[1] + bbox[3]) / 2)\n                \n                # Estimate depth (simplified - use depth camera in real robot)\n                depth = 2.0  # Assume 2 meters for demo\n                \n                # Convert to 3D\n                position = self.pixel_to_3d(center_x, center_y, depth)\n                \n                self.object_position = position\n                \n                # Publish detection\n                obj_msg = String()\n                obj_msg.data = f"{class_name} at {position}"\n                self.object_pub.publish(obj_msg)\n                \n                self.get_logger().info(f"Found {class_name} at {position}")\n                return\n        \n        self.get_logger().info(f"{self.target_object} not found")\n    \n    def ros_to_cv2(self, ros_image):\n        """Convert ROS Image message to OpenCV format"""\n        np_arr = np.frombuffer(ros_image.data, np.uint8)\n        image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n        return image\n    \n    def pixel_to_3d(self, px, py, depth):\n        """Convert pixel coordinates to 3D robot coordinates"""\n        fx = self.camera_info[\'fx\']\n        fy = self.camera_info[\'fy\']\n        cx = self.camera_info[\'cx\']\n        cy = self.camera_info[\'cy\']\n        \n        x = (px - cx) * depth / fx\n        y = (py - cy) * depth / fy\n        z = depth\n        \n        return [x, y, z]\n\n\nclass NavigationController(Node):\n    """Node 4: Navigate robot to positions"""\n    \n    def __init__(self):\n        super().__init__(\'navigation_controller\')\n        \n        # Action client for navigation\n        self.nav_client = ActionClient(\n            self, NavigateToPose, \'navigate_to_pose\'\n        )\n        \n        # Publisher for simple velocity commands\n        self.cmd_vel_pub = self.create_publisher(\n            Twist, \'/cmd_vel\', 10\n        )\n        \n        # Subscriber for object positions\n        self.object_sub = self.create_subscription(\n            String, \'/detected_objects\', self.object_callback, 10\n        )\n        \n        self.target_position = None\n    \n    def object_callback(self, msg):\n        """Handle detected object position"""\n        # Parse position from message\n        # Format: "cup at [x, y, z]"\n        try:\n            parts = msg.data.split(\' at \')\n            position_str = parts[1].strip(\'[]\')\n            position = [float(x) for x in position_str.split(\',\')]\n            self.target_position = position\n            self.get_logger().info(f"Target position: {position}")\n        except Exception as e:\n            self.get_logger().error(f"Could not parse position: {e}")\n    \n    def navigate_to_object(self):\n        """Navigate to the detected object"""\n        if self.target_position is None:\n            self.get_logger().warn("No target position set")\n            return\n        \n        self.nav_client.wait_for_server()\n        \n        # Create goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = PoseStamped()\n        goal_msg.pose.pose.position.x = self.target_position[0]\n        goal_msg.pose.pose.position.y = self.target_position[1]\n        goal_msg.pose.pose.position.z = 0.0\n        \n        self.get_logger().info("Sending navigation goal")\n        self.nav_client.send_goal_async(goal_msg)\n\n\nclass ArmController(Node):\n    """Node 5: Control robot arm for picking objects"""\n    \n    def __init__(self):\n        super().__init__(\'arm_controller\')\n        \n        # Publisher for arm joint commands\n        self.arm_pub = self.create_publisher(\n            JointTrajectory, \'/arm_controller/joint_trajectory\', 10\n        )\n        \n        # Gripper control\n        self.gripper_pub = self.create_publisher(\n            JointTrajectory, \'/gripper_controller/joint_trajectory\', 10\n        )\n        \n        # Arm state\n        self.arm_ready = True\n    \n    def pick_object(self):\n        """Execute pick motion"""\n        if not self.arm_ready:\n            return\n        \n        self.get_logger().info("Starting pick sequence")\n        \n        # Step 1: Move arm to pre-pick position\n        self.move_to_pre_pick()\n        \n        # Step 2: Lower arm to object\n        self.lower_arm()\n        \n        # Step 3: Close gripper\n        self.close_gripper()\n        \n        # Step 4: Lift arm\n        self.lift_arm()\n        \n        self.get_logger().info("Pick complete")\n    \n    def move_to_pre_pick(self):\n        """Move arm to position above object"""\n        traj = JointTrajectory()\n        traj.joint_names = [\'arm_joint_1\', \'arm_joint_2\', \'arm_joint_3\']\n        \n        point = JointTrajectoryPoint()\n        point.positions = [0.0, 0.5, -0.5]  # Example joint angles\n        point.time_from_start = rclpy.duration.Duration(seconds=2)\n        \n        traj.points.append(point)\n        self.arm_pub.publish(traj)\n    \n    def lower_arm(self):\n        """Lower arm to grasp object"""\n        traj = JointTrajectory()\n        traj.joint_names = [\'arm_joint_1\', \'arm_joint_2\', \'arm_joint_3\']\n        \n        point = JointTrajectoryPoint()\n        point.positions = [0.0, 0.2, -0.3]\n        point.time_from_start = rclpy.duration.Duration(seconds=2)\n        \n        traj.points.append(point)\n        self.arm_pub.publish(traj)\n    \n    def close_gripper(self):\n        """Close gripper to grasp object"""\n        traj = JointTrajectory()\n        traj.joint_names = [\'gripper_joint\']\n        \n        point = JointTrajectoryPoint()\n        point.positions = [0.0]  # Closed position\n        point.time_from_start = rclpy.duration.Duration(seconds=1)\n        \n        traj.points.append(point)\n        self.gripper_pub.publish(traj)\n    \n    def lift_arm(self):\n        """Lift arm with object"""\n        traj = JointTrajectory()\n        traj.joint_names = [\'arm_joint_1\', \'arm_joint_2\', \'arm_joint_3\']\n        \n        point = JointTrajectoryPoint()\n        point.positions = [0.0, 0.8, -0.8]\n        point.time_from_start = rclpy.duration.Duration(seconds=2)\n        \n        traj.points.append(point)\n        self.arm_pub.publish(traj)\n\n\nclass VLACoordinator(Node):\n    """Main coordinator node that ties all VLA components together"""\n    \n    def __init__(self):\n        super().__init__(\'vla_coordinator\')\n        \n        # Subscriber for planned steps\n        self.steps_sub = self.create_subscription(\n            String, \'/planned_steps\', self.steps_callback, 10\n        )\n        \n        # Create component nodes\n        self.vision = VisionSystem()\n        self.navigation = NavigationController()\n        self.arm = ArmController()\n        \n        # Current step\n        self.current_step = 0\n        self.steps = []\n    \n    def steps_callback(self, msg):\n        """Process planned steps and execute them"""\n        steps_str = msg.data\n        self.steps = steps_str.split()\n        self.current_step = 0\n        \n        self.get_logger().info(f"Executing {len(self.steps)} steps")\n        \n        # Execute first step\n        self.execute_next_step()\n    \n    def execute_next_step(self):\n        """Execute the next step in the plan"""\n        if self.current_step >= len(self.steps):\n            self.get_logger().info("All steps complete!")\n            return\n        \n        step = self.steps[self.current_step]\n        self.get_logger().info(f"Executing: {step}")\n        \n        # Parse and execute step\n        if \'find_\' in step:\n            object_name = step.replace(\'find_\', \'\')\n            self.vision.set_target(object_name)\n            \n        elif \'navigate_\' in step:\n            self.navigation.navigate_to_object()\n            \n        elif \'pick\' in step:\n            self.arm.pick_object()\n            \n        elif \'release\' in step:\n            self.arm.open_gripper()\n            \n        elif \'move_forward\' in step:\n            self.send_velocity(0.5, 0.0)\n            \n        elif \'turn_left\' in step:\n            self.send_velocity(0.0, 0.5)\n            \n        elif \'turn_right\' in step:\n            self.send_velocity(0.0, -0.5)\n            \n        elif \'stop\' in step:\n            self.send_velocity(0.0, 0.0)\n        \n        # Move to next step after delay (in real robot, wait for completion)\n        self.create_timer(2.0, self.increment_step)\n    \n    def increment_step(self):\n        """Move to next step"""\n        self.current_step += 1\n        self.execute_next_step()\n    \n    def send_velocity(self, linear, angular):\n        """Send velocity command"""\n        msg = Twist()\n        msg.linear.x = linear\n        msg.angular.z = angular\n        self.navigation.cmd_vel_pub.publish(msg)\n\n\ndef main():\n    """Main entry point for VLA Humanoid Robot"""\n    rclpy.init()\n    \n    # Create all nodes\n    speech_listener = SpeechListener()\n    llm_planner = LLMPlanner()\n    vla_coordinator = VLACoordinator()\n    \n    # Use multi-threaded executor to run all nodes\n    executor = MultiThreadedExecutor()\n    executor.add_node(speech_listener)\n    executor.add_node(llm_planner)\n    executor.add_node(vla_coordinator)\n    \n    # Spin all nodes\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        executor.shutdown()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"running-the-capstone-project",children:"Running the Capstone Project"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Install dependencies"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install SpeechRecognition pyaudio ultralytics opencv-python\npip install rclpy nav2_msgs trajectory_msgs\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Download YOLO model"})," (first time only):"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\nmodel = YOLO('yolov8n.pt')  # Auto-downloads on first run\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Launch the robot"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python vla_humanoid_robot.py\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"4",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Give a voice command"}),":"]}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Say: "Pick up the cup"'}),"\n",(0,s.jsxs)(n.li,{children:["The robot will:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Hear your command"}),"\n",(0,s.jsx)(n.li,{children:"Plan the steps"}),"\n",(0,s.jsx)(n.li,{children:"Find the cup with its camera"}),"\n",(0,s.jsx)(n.li,{children:"Navigate to the cup"}),"\n",(0,s.jsx)(n.li,{children:"Pick it up"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"testing-without-hardware",children:"Testing Without Hardware"}),"\n",(0,s.jsx)(n.p,{children:"You can test the system with simulated data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Test speech recognition with audio file\naudio = sr.AudioFile('test_command.wav')\nwith audio as source:\n    audio_data = recognizer.record(source)\n    command = recognizer.recognize_google(audio_data)\n\n# Test vision with image file\nimage = cv2.imread('test_room.jpg')\nresults = model(image)\n\n# Test navigation with mock positions\nnavigation.target_position = [1.5, 0.3, 0.0]\nnavigation.navigate_to_object()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(n.p,{children:["In this module, you learned about ",(0,s.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," systems:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Topic"}),(0,s.jsx)(n.th,{children:"What You Learned"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"VLA Basics"})}),(0,s.jsx)(n.td,{children:"How robots combine vision, language, and action"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Speech-to-Text"})}),(0,s.jsx)(n.td,{children:"Converting voice commands to text"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"LLM Planning"})}),(0,s.jsx)(n.td,{children:"Breaking commands into robot steps"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Computer Vision"})}),(0,s.jsx)(n.td,{children:"Detecting objects with YOLO"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"ROS 2 Actions"})}),(0,s.jsx)(n.td,{children:"Using actions for long-running tasks"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Capstone Project"})}),(0,s.jsx)(n.td,{children:"A complete humanoid robot assistant"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VLA"})," combines three abilities: seeing, understanding, and acting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech recognition"})," lets robots hear human commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLMs"})," convert vague commands into specific steps"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computer vision"})," helps robots find and identify objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 actions"})," control complex robot behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration"})," of all components creates intelligent robots"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"whats-next",children:"What's Next?"}),"\n",(0,s.jsx)(n.p,{children:"You now have the foundation to build smart, autonomous robots! Try:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adding more voice commands"}),"\n",(0,s.jsx)(n.li,{children:"Training a custom object detector"}),"\n",(0,s.jsx)(n.li,{children:"Implementing better navigation with SLAM"}),"\n",(0,s.jsx)(n.li,{children:"Adding obstacle avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Creating multi-step task planning"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Congratulations on completing Module 4!"})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>l});var o=t(6540);const s={},i=o.createContext(s);function r(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);