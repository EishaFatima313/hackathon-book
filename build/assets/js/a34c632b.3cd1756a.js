"use strict";(globalThis.webpackChunkrobotics_education_platform=globalThis.webpackChunkrobotics_education_platform||[]).push([[481],{6457(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"digital-twin/sensor-simulation","title":"Chapter 3: Sensor Simulation & Testing","description":"Overview","source":"@site/docs/digital-twin/sensor-simulation.md","sourceDirName":"digital-twin","slug":"/digital-twin/sensor-simulation","permalink":"/docs/digital-twin/sensor-simulation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"Chapter 2: Digital Twin in Unity","permalink":"/docs/digital-twin/digital-twin-unity"},"next":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","permalink":"/docs/ai-robot-brain/"}}');var r=i(4848),a=i(8453);const t={sidebar_position:3},o="Chapter 3: Sensor Simulation & Testing",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Understanding Robot Sensors",id:"understanding-robot-sensors",level:2},{value:"Types of Sensors",id:"types-of-sensors",level:3},{value:"Why Simulate Sensors?",id:"why-simulate-sensors",level:3},{value:"LiDAR Simulation in Gazebo",id:"lidar-simulation-in-gazebo",level:2},{value:"What is LiDAR?",id:"what-is-lidar",level:3},{value:"Adding LiDAR to Your Robot",id:"adding-lidar-to-your-robot",level:3},{value:"LiDAR Output",id:"lidar-output",level:3},{value:"Camera Simulation in Gazebo",id:"camera-simulation-in-gazebo",level:2},{value:"Adding a Camera to Your Robot",id:"adding-a-camera-to-your-robot",level:3},{value:"Camera Output",id:"camera-output",level:3},{value:"IMU Simulation in Gazebo",id:"imu-simulation-in-gazebo",level:2},{value:"What is an IMU?",id:"what-is-an-imu",level:3},{value:"Adding IMU to Your Robot",id:"adding-imu-to-your-robot",level:3},{value:"IMU Output",id:"imu-output",level:3},{value:"Visualizing Sensors in Unity",id:"visualizing-sensors-in-unity",level:2},{value:"LiDAR Visualization",id:"lidar-visualization",level:3},{value:"Camera Visualization in Unity",id:"camera-visualization-in-unity",level:3},{value:"Testing Sensor Performance",id:"testing-sensor-performance",level:2},{value:"Creating Test Scenarios",id:"creating-test-scenarios",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3},{value:"Practice Task 1: Sensor Integration",id:"practice-task-1-sensor-integration",level:2},{value:"Practice Task 2: Unity Sensor Visualization",id:"practice-task-2-unity-sensor-visualization",level:2},{value:"Advanced Sensor Fusion",id:"advanced-sensor-fusion",level:2},{value:"Combining Multiple Sensors",id:"combining-multiple-sensors",level:3},{value:"Kalman Filters",id:"kalman-filters",level:3},{value:"Debugging Sensor Issues",id:"debugging-sensor-issues",level:2},{value:"Common Problems",id:"common-problems",level:3},{value:"Diagnostic Tools",id:"diagnostic-tools",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"chapter-3-sensor-simulation--testing",children:"Chapter 3: Sensor Simulation & Testing"})}),"\n",(0,r.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(e.p,{children:"Welcome to Chapter 3! In this chapter, we'll dive deep into sensor simulation, which is crucial for robot perception. We'll learn how to simulate real-world sensors like LiDAR, cameras, and IMUs in Gazebo, visualize them in Unity, and test our robots' perception capabilities. Think of sensors as your robot's eyes and ears!"}),"\n",(0,r.jsx)(e.h2,{id:"understanding-robot-sensors",children:"Understanding Robot Sensors"}),"\n",(0,r.jsx)(e.h3,{id:"types-of-sensors",children:"Types of Sensors"}),"\n",(0,r.jsx)(e.p,{children:"Robots use various sensors to perceive their environment:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Cameras"}),": Visual information (RGB, depth, stereo)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"LiDAR"}),": Distance measurements using laser light"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"IMU"}),": Inertial measurement unit (acceleration, rotation)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GPS"}),": Global positioning"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sonar/Ultrasonic"}),": Short-range distance sensing"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Force/Torque"}),": Physical interaction detection"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"why-simulate-sensors",children:"Why Simulate Sensors?"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Safety"}),": Test perception algorithms safely"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Variety"}),": Test with different sensor configurations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Scenarios"}),": Create challenging situations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Cost"}),": No need for expensive hardware"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Repeatability"}),": Same conditions for testing"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"lidar-simulation-in-gazebo",children:"LiDAR Simulation in Gazebo"}),"\n",(0,r.jsx)(e.h3,{id:"what-is-lidar",children:"What is LiDAR?"}),"\n",(0,r.jsx)(e.p,{children:'LiDAR (Light Detection and Ranging) uses laser pulses to measure distances. It creates a "point cloud" of the environment.'}),"\n",(0,r.jsx)(e.h3,{id:"adding-lidar-to-your-robot",children:"Adding LiDAR to Your Robot"}),"\n",(0,r.jsx)(e.p,{children:"In your URDF, add a LiDAR sensor:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- LiDAR Link --\x3e\n<link name="lidar_link">\n  <visual>\n    <geometry>\n      <cylinder radius="0.05" length="0.05"/>\n    </geometry>\n    <material name="green">\n      <color rgba="0 1 0 1"/>\n    </material>\n  </visual>\n  <collision>\n    <geometry>\n      <cylinder radius="0.05" length="0.05"/>\n    </geometry>\n  </collision>\n  <inertial>\n    <mass value="0.1"/>\n    <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>\n  </inertial>\n</link>\n\n\x3c!-- LiDAR Joint --\x3e\n<joint name="lidar_joint" type="fixed">\n  <parent link="base_link"/>\n  <child link="lidar_link"/>\n  <origin xyz="0.0 0.0 0.2" rpy="0 0 0"/>\n</joint>\n\n\x3c!-- Gazebo Plugin for LiDAR --\x3e\n<gazebo reference="lidar_link">\n  <sensor type="ray" name="lidar_sensor">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>360</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/lidar</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(e.h3,{id:"lidar-output",children:"LiDAR Output"}),"\n",(0,r.jsxs)(e.p,{children:["The LiDAR publishes to ",(0,r.jsx)(e.code,{children:"/lidar/scan"})," topic with ",(0,r.jsx)(e.code,{children:"sensor_msgs/LaserScan"})," message containing:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"ranges[]"}),": Array of distance measurements"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"intensities[]"}),": Reflectivity values"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"angle_min"}),", ",(0,r.jsx)(e.code,{children:"angle_max"}),": Angular range"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"angle_increment"}),": Angular resolution"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"time_increment"}),": Time between measurements"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"camera-simulation-in-gazebo",children:"Camera Simulation in Gazebo"}),"\n",(0,r.jsx)(e.h3,{id:"adding-a-camera-to-your-robot",children:"Adding a Camera to Your Robot"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Camera Link --\x3e\n<link name="camera_link">\n  <visual>\n    <geometry>\n      <box size="0.05 0.1 0.05"/>\n    </geometry>\n    <material name="black">\n      <color rgba="0 0 0 1"/>\n    </material>\n  </visual>\n  <collision>\n    <geometry>\n      <box size="0.05 0.1 0.05"/>\n    </geometry>\n  </collision>\n  <inertial>\n    <mass value="0.01"/>\n    <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n  </inertial>\n</link>\n\n\x3c!-- Camera Joint --\x3e\n<joint name="camera_joint" type="fixed">\n  <parent link="base_link"/>\n  <child link="camera_link"/>\n  <origin xyz="0.1 0.0 0.1" rpy="0 0 0"/>\n</joint>\n\n\x3c!-- Gazebo Plugin for Camera --\x3e\n<gazebo reference="camera_link">\n  <sensor type="camera" name="camera_sensor">\n    <update_rate>30</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.3962634</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>/camera</namespace>\n        <remapping>image_raw:=image_color</remapping>\n      </ros>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(e.h3,{id:"camera-output",children:"Camera Output"}),"\n",(0,r.jsxs)(e.p,{children:["The camera publishes to ",(0,r.jsx)(e.code,{children:"/camera/image_color"})," topic with ",(0,r.jsx)(e.code,{children:"sensor_msgs/Image"})," message containing:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"data[]"}),": Raw pixel data"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"encoding"}),": Pixel format (rgb8, bgr8, etc.)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"width"}),", ",(0,r.jsx)(e.code,{children:"height"}),": Image dimensions"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"step"}),": Bytes per row"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"imu-simulation-in-gazebo",children:"IMU Simulation in Gazebo"}),"\n",(0,r.jsx)(e.h3,{id:"what-is-an-imu",children:"What is an IMU?"}),"\n",(0,r.jsx)(e.p,{children:"An Inertial Measurement Unit measures:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Linear acceleration (x, y, z)"}),"\n",(0,r.jsx)(e.li,{children:"Angular velocity (roll, pitch, yaw)"}),"\n",(0,r.jsx)(e.li,{children:"Sometimes orientation (with magnetometer)"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"adding-imu-to-your-robot",children:"Adding IMU to Your Robot"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- IMU Link --\x3e\n<link name="imu_link">\n  <visual>\n    <geometry>\n      <box size="0.02 0.02 0.02"/>\n    </geometry>\n  </visual>\n  <collision>\n    <geometry>\n      <box size="0.02 0.02 0.02"/>\n    </geometry>\n  </collision>\n  <inertial>\n    <mass value="0.01"/>\n    <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n  </inertial>\n</link>\n\n\x3c!-- IMU Joint --\x3e\n<joint name="imu_joint" type="fixed">\n  <parent link="base_link"/>\n  <child link="imu_link"/>\n  <origin xyz="0.0 0.0 0.1" rpy="0 0 0"/>\n</joint>\n\n\x3c!-- Gazebo Plugin for IMU --\x3e\n<gazebo reference="imu_link">\n  <sensor type="imu" name="imu_sensor">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <visualize>false</visualize>\n    <topic>__default_topic__</topic>\n    <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n      <ros>\n        <namespace>/imu</namespace>\n        <remapping>~/out:=data</remapping>\n      </ros>\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(e.h3,{id:"imu-output",children:"IMU Output"}),"\n",(0,r.jsxs)(e.p,{children:["The IMU publishes to ",(0,r.jsx)(e.code,{children:"/imu/data"})," topic with ",(0,r.jsx)(e.code,{children:"sensor_msgs/Imu"})," message containing:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"orientation"}),": Quaternion representing orientation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"angular_velocity"}),": Angular velocity vector"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"linear_acceleration"}),": Linear acceleration vector"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"visualizing-sensors-in-unity",children:"Visualizing Sensors in Unity"}),"\n",(0,r.jsx)(e.h3,{id:"lidar-visualization",children:"LiDAR Visualization"}),"\n",(0,r.jsx)(e.p,{children:"In Unity, you can visualize LiDAR data as:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Point clouds"}),"\n",(0,r.jsx)(e.li,{children:"Line segments"}),"\n",(0,r.jsx)(e.li,{children:"3D mesh reconstruction"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Example Unity script for LiDAR visualization:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing RosSharp.RosBridgeClient;\nusing RosSharp.Messages.Sensor;\n\npublic class LidarVisualizer : MonoBehaviour\n{\n    public GameObject pointPrefab;\n    private GameObject[] points;\n    private LaserScan laserScanMsg;\n\n    void Start()\n    {\n        RosConnector.Instance.Subscribe<LaserScan>("/lidar/scan", OnLidarReceived);\n    }\n\n    void OnLidarReceived(LaserScan msg)\n    {\n        laserScanMsg = msg;\n        UpdateVisualization();\n    }\n\n    void UpdateVisualization()\n    {\n        if (laserScanMsg == null) return;\n\n        // Clean up previous points\n        if (points != null)\n        {\n            foreach (GameObject point in points)\n            {\n                if (point != null) DestroyImmediate(point);\n            }\n        }\n\n        // Create new points for each laser reading\n        points = new GameObject[laserScanMsg.ranges.Length];\n        \n        for (int i = 0; i < laserScanMsg.ranges.Length; i++)\n        {\n            float distance = laserScanMsg.ranges[i];\n            \n            // Skip invalid readings\n            if (distance < laserScanMsg.range_min || distance > laserScanMsg.range_max)\n                continue;\n\n            float angle = laserScanMsg.angle_min + i * laserScanMsg.angle_increment;\n            \n            // Calculate position in 2D (for top-down view)\n            float x = distance * Mathf.Cos(angle);\n            float y = distance * Mathf.Sin(angle);\n            \n            Vector3 position = new Vector3(x, 0.1f, y); // Slightly above ground\n            \n            points[i] = Instantiate(pointPrefab, position, Quaternion.identity);\n            points[i].transform.localScale = Vector3.one * 0.05f; // Small point size\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"camera-visualization-in-unity",children:"Camera Visualization in Unity"}),"\n",(0,r.jsx)(e.p,{children:"To visualize camera data in Unity:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Receive image data from ROS"}),"\n",(0,r.jsx)(e.li,{children:"Convert to Unity texture"}),"\n",(0,r.jsx)(e.li,{children:"Display on a material or UI element"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Example Unity camera visualization:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing RosSharp.RosBridgeClient;\nusing RosSharp.Messages.Sensor;\n\npublic class CameraVisualizer : MonoBehaviour\n{\n    public Renderer targetRenderer;\n    private Texture2D cameraTexture;\n\n    void Start()\n    {\n        RosConnector.Instance.Subscribe<Image>("/camera/image_color", OnCameraReceived);\n    }\n\n    void OnCameraReceived(Image msg)\n    {\n        // Convert ROS image to Unity texture\n        if (cameraTexture == null)\n        {\n            cameraTexture = new Texture2D((int)msg.width, (int)msg.height, TextureFormat.RGB24, false);\n            if (targetRenderer != null)\n            {\n                targetRenderer.material.mainTexture = cameraTexture;\n            }\n        }\n\n        // Update texture with new image data\n        cameraTexture.LoadRawTextureData(msg.data);\n        cameraTexture.Apply();\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"testing-sensor-performance",children:"Testing Sensor Performance"}),"\n",(0,r.jsx)(e.h3,{id:"creating-test-scenarios",children:"Creating Test Scenarios"}),"\n",(0,r.jsx)(e.p,{children:"Design specific scenarios to test sensor capabilities:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Obstacle Detection"}),": Place objects at known distances"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Mapping"}),": Navigate through structured environments"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Localization"}),": Test position estimation in known maps"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Dynamic Objects"}),": Moving obstacles to test tracking"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,r.jsx)(e.p,{children:"Common metrics for sensor performance:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Accuracy"}),": How close measurements are to true values"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Precision"}),": Consistency of repeated measurements"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Range"}),": Maximum and minimum detectable distances"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Resolution"}),": Smallest distinguishable difference"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Update Rate"}),": How frequently data is published"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"practice-task-1-sensor-integration",children:"Practice Task 1: Sensor Integration"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Add LiDAR, camera, and IMU to your robot URDF"}),"\n",(0,r.jsx)(e.li,{children:"Launch Gazebo with your sensor-equipped robot"}),"\n",(0,r.jsx)(e.li,{children:"Verify that sensor topics are publishing data"}),"\n",(0,r.jsxs)(e.li,{children:["Use ",(0,r.jsx)(e.code,{children:"ros2 topic echo"})," to check sensor messages"]}),"\n",(0,r.jsx)(e.li,{children:"Create a simple Python script to subscribe to sensor data"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Example Python sensor subscriber:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass SensorTester(Node):\n    def __init__(self):\n        super().__init__('sensor_tester')\n        \n        # Create subscribers for each sensor\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/lidar/scan', self.lidar_callback, 10)\n        \n        self.camera_sub = self.create_subscription(\n            Image, '/camera/image_color', self.camera_callback, 10)\n        \n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n        \n        self.bridge = CvBridge()\n        self.get_logger().info('Sensor tester initialized')\n\n    def lidar_callback(self, msg):\n        # Find closest obstacle\n        min_distance = min([r for r in msg.ranges if r > msg.range_min])\n        self.get_logger().info(f'Lidar: Closest obstacle at {min_distance:.2f}m')\n\n    def camera_callback(self, msg):\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        height, width, channels = cv_image.shape\n        self.get_logger().info(f'Camera: Received {width}x{height} image')\n\n    def imu_callback(self, msg):\n        # Log orientation\n        orientation = msg.orientation\n        self.get_logger().info(f'IMU: Orientation ({orientation.x:.2f}, {orientation.y:.2f}, {orientation.z:.2f}, {orientation.w:.2f})')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_tester = SensorTester()\n    rclpy.spin(sensor_tester)\n    sensor_tester.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h2,{id:"practice-task-2-unity-sensor-visualization",children:"Practice Task 2: Unity Sensor Visualization"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Create visualization elements in Unity for each sensor"}),"\n",(0,r.jsx)(e.li,{children:"Connect to ROS topics using ROS#"}),"\n",(0,r.jsx)(e.li,{children:"Display LiDAR points in 3D space"}),"\n",(0,r.jsx)(e.li,{children:"Show camera feed on a screen in Unity"}),"\n",(0,r.jsx)(e.li,{children:"Visualize IMU orientation with arrows"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"advanced-sensor-fusion",children:"Advanced Sensor Fusion"}),"\n",(0,r.jsx)(e.h3,{id:"combining-multiple-sensors",children:"Combining Multiple Sensors"}),"\n",(0,r.jsx)(e.p,{children:"Real robots often combine data from multiple sensors:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"LiDAR + Camera"}),": Object recognition with accurate distances"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"IMU + Wheel Encoders"}),": More accurate position estimation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GPS + IMU"}),": Outdoor navigation with drift correction"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"kalman-filters",children:"Kalman Filters"}),"\n",(0,r.jsx)(e.p,{children:"Kalman filters help combine sensor data optimally:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Predict robot state based on motion"}),"\n",(0,r.jsx)(e.li,{children:"Correct prediction with sensor measurements"}),"\n",(0,r.jsx)(e.li,{children:"Account for sensor uncertainties"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"debugging-sensor-issues",children:"Debugging Sensor Issues"}),"\n",(0,r.jsx)(e.h3,{id:"common-problems",children:"Common Problems"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"No data"}),": Check topic connections and permissions"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Wrong data"}),": Verify coordinate frames and units"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Delayed data"}),": Check network latency and processing time"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Noise"}),": Increase sensor resolution or add filtering"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"diagnostic-tools",children:"Diagnostic Tools"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"ros2 run rqt_plot rqt_plot"}),": Plot sensor values over time"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"rviz2"}),": Visualize sensor data in 3D"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"ros2 topic hz"}),": Check topic update rate"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"ros2 bag record"}),": Record data for offline analysis"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"How to simulate different types of sensors in Gazebo"}),"\n",(0,r.jsx)(e.li,{children:"How to visualize sensor data in Unity"}),"\n",(0,r.jsx)(e.li,{children:"How to test and evaluate sensor performance"}),"\n",(0,r.jsx)(e.li,{children:"How to debug common sensor issues"}),"\n",(0,r.jsx)(e.li,{children:"Basic concepts of sensor fusion"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(e.p,{children:"Congratulations! You've completed the Digital Twin module. You now understand how to:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Create robot simulations in Gazebo"}),"\n",(0,r.jsx)(e.li,{children:"Build digital twins in Unity"}),"\n",(0,r.jsx)(e.li,{children:"Simulate and test various sensors"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Continue exploring robotics by combining these skills with AI and machine learning techniques!"})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>t,x:()=>o});var s=i(6540);const r={},a=s.createContext(r);function t(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);